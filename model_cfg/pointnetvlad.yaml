

optimizer:
  lr:         0.001               # sgd learning rate
  wup_epochs: 1          # warmup during first XX epochs (can be float)
  momentum:   0.9          # sgd momentum
  lr_decay:   0.99         # learning rate decay per epoch after initial cycle (from min lr)
  w_decay:    0.00001       # weight decay
  betas:      [0.9, 0.99]
  epsilon_w:  0.00001
  amsgrad:    True


device: 'cuda:1'

max_points: 10000

################################################################################
# backbone parameters
################################################################################


